{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24269,"status":"ok","timestamp":1696476847992,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"mPyflgaTFKWy","outputId":"e8b02edd-9d01-4dcc-fcee-6412a9891f64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4613,"status":"ok","timestamp":1696476852602,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"o7CUi7JYIX77"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import torchvision"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696476852602,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"u_kEpn4ffdGh"},"outputs":[],"source":["from torchvision.models import vgg16, VGG16_Weights"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1696476852602,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"MarrvykCQrXr","outputId":"1a7d40e3-d9d9-498f-e5fb-677eb0e3da91"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/AIDTransformer\n"]}],"source":["%cd /content/drive/MyDrive/AIDTransformer"]},{"cell_type":"markdown","metadata":{"id":"kYUcrZfYPO34"},"source":["# Load Data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3516,"status":"ok","timestamp":1696476856114,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"1pmsFwJFPd1_"},"outputs":[],"source":["from dataset import *\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1696476856115,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"S3mz4qFgPQg5"},"outputs":[],"source":["Dataset = '/content/drive/MyDrive/AIDTransformer/Dataset/Training_data/RICE'"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9523,"status":"ok","timestamp":1696476865620,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"fJK07oWqZGdy"},"outputs":[],"source":["# Create the data loader\n","data_loader = DataLoaderTrain(rgb_dir=Dataset, img_options={'patch_size': 256})\n","\n","# Create the data loader\n","# data_loader = DataLoader(dataset, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":34,"status":"ok","timestamp":1696476865620,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"YWWWWJJRqB5M"},"outputs":[],"source":["# import cv2\n","# from google.colab.patches import cv2_imshow\n","\n","# path = '/content/drive/MyDrive/AIDTransformer/testing_data/RICE/input/0.png'\n","\n","# img = cv2.imread(path)\n","# img.shape\n","# #cv2_imshow(img)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1696476865621,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"ojp21fIEwDYI","outputId":"3492744b-a8af-43c7-f0aa-47cf35be45a0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<dataset.DataLoaderTrain at 0x7e2aec5c16f0>"]},"metadata":{},"execution_count":9}],"source":["data_loader"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1696476865621,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"Gf6G5pbH6zvE","outputId":"0fe95165-ee40-4793-a795-c7e21ef7a69f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["736"]},"metadata":{},"execution_count":10}],"source":["len(data_loader)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1696476865621,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"0_FejSIjqjOa"},"outputs":[],"source":["# data_loader[1].shape"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2624,"status":"ok","timestamp":1696476868221,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"qyuB_PJ9cLGa","outputId":"087ef733-c8f9-4a12-e51e-e9e4b5735512"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","4\n"]}],"source":["print(len(data_loader[499]))"]},{"cell_type":"markdown","metadata":{"id":"D02Cn4jtOuQP"},"source":["# Model"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6753,"status":"ok","timestamp":1696476874959,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"IDgXtpfgdrVt","outputId":"c997ab63-bb35-4e04-a108-d8e601d8ab68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting timm\n","  Downloading timm-0.9.7-py3-none-any.whl (2.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Collecting huggingface-hub (from timm)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.27.5)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (17.0.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.66.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.23.5)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.5)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.3.3 timm-0.9.7\n"]}],"source":["!pip install timm"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5922,"status":"ok","timestamp":1696476880873,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"x-xOIRUWd5_F","outputId":"9964049a-9646-4208-ab8c-4c09dbff0a09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting einops\n","  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.7.0\n"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1420,"status":"ok","timestamp":1696476882289,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"ayrvIv_tcUif"},"outputs":[],"source":["from model import *"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":591,"status":"ok","timestamp":1696476882870,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"4A8jIFvDJJqL","outputId":"735a9715-948e-4144-f36a-d5854ce0c346"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["model = Network()"]},{"cell_type":"markdown","metadata":{"id":"D4MvvP5gM4On"},"source":["# perpectual loss"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12373,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"TpmD7w5afbCM","outputId":"bb9ebff8-d773-497b-966d-a7054ea36d1c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100%|██████████| 528M/528M [00:02<00:00, 227MB/s]\n"]}],"source":["# Load VGG16 with ImageNet weights\n","#model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n","\n","# Or use the most up-to-date weights\n","vgg_model = vgg16(weights=VGG16_Weights.DEFAULT)\n","\n","if torch.cuda.is_available():\n","    vgg_model.cuda()\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"KalNgyifiMLe"},"outputs":[],"source":["from collections import namedtuple"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"5h_hW2X6iEpl"},"outputs":[],"source":["LossOutput = namedtuple(\"LossOutput\", [\"relu1_2\", \"relu2_2\", \"relu3_3\"])\n","\n","class LossNetwork(torch.nn.Module):\n","    def __init__(self, vgg_model):\n","        super(LossNetwork, self).__init__()\n","        self.vgg_layers = vgg_model.features\n","        self.layer_name_mapping = {\n","            '3': \"relu1_2\",\n","            '8': \"relu2_2\",\n","            '15': \"relu3_3\"\n","        }\n","\n","    def forward(self, x):\n","        output = {}\n","        for name, module in self.vgg_layers._modules.items():\n","            x = module(x)\n","            if name in self.layer_name_mapping:\n","                output[self.layer_name_mapping[name]] = x\n","        return LossOutput(**output)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"VthXXK-nWnBe"},"outputs":[],"source":["# Assuming output and ground_truth are your output and ground truth images\n","loss_network = LossNetwork(vgg_model)\n","loss_network.eval()\n","def perpectual_loss(y_pred, y_train):\n","  output_features = loss_network(y_pred)\n","  ground_truth_features = loss_network(y_train)\n","\n","  # Calculate the perceptual loss\n","  perceptual_loss = 0\n","  for output_feature, ground_truth_feature in zip(output_features, ground_truth_features):\n","      perceptual_loss += torch.nn.functional.l1_loss(output_feature, ground_truth_feature)\n","  return perceptual_loss"]},{"cell_type":"markdown","metadata":{"id":"jF7RtGHLM9dY"},"source":["# edge loss"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"aPSXpX20beVu"},"outputs":[],"source":["import cv2 as cv"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"5BbK9qg-i2Jt"},"outputs":[],"source":["def sobel_edge_loss(img):\n","    #print('sobel')\n","    print(img.shape)\n","    #print(type(img))\n","    #img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n","    sobel_kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).expand(-1, 3, -1, -1)\n","    sobel_kernel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]], dtype=torch.float32).unsqueeze(0).unsqueeze(0).expand(-1, 3, -1, -1)\n","\n","    sobel_kernel_x = sobel_kernel_x.to(img.device)\n","    sobel_kernel_y = sobel_kernel_y.to(img.device)\n","\n","    edges_x = F.conv2d(img, sobel_kernel_x, padding=0)\n","    edges_y = F.conv2d(img, sobel_kernel_y, padding=0)\n","\n","    edges = torch.sqrt(edges_x**2 + edges_y**2)\n","\n","    return edges"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"ueEpqIgAi-PQ"},"outputs":[],"source":["def edge_loss(output, target):\n","\n","    output_edges = sobel_edge_loss(output)\n","    target_edges = sobel_edge_loss(target)\n","\n","    loss = F.l1_loss(output_edges, target_edges)\n","\n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"otE9w6xQNDRu"},"source":["# L1 Loss và Total Loss"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"aZmBgenuMg4h"},"outputs":[],"source":["import torch.nn.functional as F"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696476895241,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"1J6lrFmMJRP5"},"outputs":[],"source":["L1_Loss = nn.L1Loss()\n","def loss_function(y_pred, y_train):\n","  loss1 = L1_Loss(y_pred, y_train)\n","  loss2 = edge_loss(y_pred, y_train)\n","  loss3 = perpectual_loss(y_pred, y_train)\n","\n","  total_loss = 1*loss1 + 5*loss2 + 10*loss3\n","  return total_loss"]},{"cell_type":"markdown","metadata":{"id":"lebLsDoBa_AW"},"source":["# Training"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1696476895865,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"JLu9jyE9jATP","outputId":"83db20ca-2946-48bc-d8bc-5cb3c95c49c2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Network(\n","  embed_dim=32, token_projection=conv, token_mlp=ffn,win_size=8\n","  (pos_drop): Dropout(p=0.0, inplace=False)\n","  (input_proj): InputProj(\n","    (proj): Sequential(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (1): LeakyReLU(negative_slope=0.01, inplace=True)\n","    )\n","  )\n","  (output_proj): OutputProj(\n","    (proj): Sequential(\n","      (0): Conv2d(96, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (output_proj1): OutputProj(\n","    (proj): Sequential(\n","      (0): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    )\n","  )\n","  (encoderlayer_0): TRANSFORMER_BLOCK(\n","    dim=32, input_resolution=(128, 128), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=1\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(128, 128), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=1\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.014)\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (dowsample_0): Downsample(\n","    (conv): Sequential(\n","      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n","  (hs_downsample): Downsample(\n","    (conv): Sequential(\n","      (0): Conv2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n","  (hs_upsample): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(96, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (qs_upsample): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(160, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (qs_upsample1): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(32, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (upsample_poolup1): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(96, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (upsample_poolup2): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(160, 96, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (encoderlayer_1): TRANSFORMER_BLOCK(\n","    dim=96, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (dowsample_1): Downsample(\n","    (conv): Sequential(\n","      (0): Conv2d(96, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n","  (encoderlayer_2): TRANSFORMER_BLOCK(\n","    dim=160, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (dowsample_2): Downsample(\n","    (conv): Sequential(\n","      (0): Conv2d(160, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n","  (encoderlayer_3): TRANSFORMER_BLOCK(\n","    dim=256, input_resolution=(16, 16), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=256, win_size=(8, 8), num_heads=8\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=256, out_features=256, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.086)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=256, input_resolution=(16, 16), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=256, win_size=(8, 8), num_heads=8\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=256, out_features=256, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.100)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (dowsample_3): Downsample(\n","    (conv): Sequential(\n","      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n","    )\n","  )\n","  (conv): TRANSFORMER_BLOCK(\n","    dim=288, input_resolution=(8, 8), depth=2\n","    (blocks): ModuleList(\n","      (0-1): 2 x Deformable_Attentive_Transformer(\n","        dim=288, input_resolution=(8, 8), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=288, win_size=(8, 8), num_heads=16\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n","              (pointwise): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(288, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n","              (pointwise): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(288, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=288)\n","              (pointwise): Conv2d(288, 288, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(288, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=288, out_features=288, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.100)\n","        (norm2): LayerNorm((288,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=288, out_features=1152, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1152, out_features=288, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (upsample_0): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(288, 256, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (decoderlayer_0): TRANSFORMER_BLOCK(\n","    dim=448, input_resolution=(16, 16), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=448, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=448, win_size=(8, 8), num_heads=16\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=448, out_features=448, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.100)\n","        (norm2): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=448, out_features=1792, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1792, out_features=448, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=448, input_resolution=(16, 16), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=448, win_size=(8, 8), num_heads=16\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)\n","              (pointwise): Conv2d(448, 448, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(448, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=448, out_features=448, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.086)\n","        (norm2): LayerNorm((448,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=448, out_features=1792, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1792, out_features=448, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (upsample_1): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(448, 128, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (decoderlayer_1): TRANSFORMER_BLOCK(\n","    dim=256, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=256, win_size=(8, 8), num_heads=8\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=256, out_features=256, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=256, input_resolution=(32, 32), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=256, win_size=(8, 8), num_heads=8\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n","              (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(256, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=256, out_features=256, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (upsample_2): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (decoderlayer_2): TRANSFORMER_BLOCK(\n","    dim=96, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (upsample_3): Upsample(\n","    (deconv): Sequential(\n","      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))\n","    )\n","  )\n","  (decoderlayer_3): TRANSFORMER_BLOCK(\n","    dim=64, input_resolution=(128, 128), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=64, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=64, out_features=64, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.014)\n","        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=64, out_features=256, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=256, out_features=64, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=64, input_resolution=(128, 128), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=64, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n","              (pointwise): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(64, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=64, out_features=64, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): Identity()\n","        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=64, out_features=256, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=256, out_features=64, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (hs1): TRANSFORMER_BLOCK(\n","    dim=32, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (hs2): TRANSFORMER_BLOCK(\n","    dim=96, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (hs3): TRANSFORMER_BLOCK(\n","    dim=96, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (hs4): TRANSFORMER_BLOCK(\n","    dim=96, input_resolution=(64, 64), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.029)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=96, input_resolution=(64, 64), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=96, win_size=(8, 8), num_heads=2\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n","              (pointwise): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(96, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=96, out_features=96, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.043)\n","        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=96, out_features=384, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=384, out_features=96, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (qs1): TRANSFORMER_BLOCK(\n","    dim=32, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=32, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=32, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n","              (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(32, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=32, out_features=32, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=32, out_features=128, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=128, out_features=32, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (qs2): TRANSFORMER_BLOCK(\n","    dim=160, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (qs3): TRANSFORMER_BLOCK(\n","    dim=160, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (qs4): TRANSFORMER_BLOCK(\n","    dim=160, input_resolution=(32, 32), depth=2\n","    (blocks): ModuleList(\n","      (0): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.057)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","      (1): Deformable_Attentive_Transformer(\n","        dim=160, input_resolution=(32, 32), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0\n","        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (attn): WindowAttention(\n","          dim=160, win_size=(8, 8), num_heads=4\n","          (qkv): ConvProjection(\n","            (to_q): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_k): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","            (to_v): SepConv2d(\n","              (depthwise): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160)\n","              (pointwise): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","              (act_layer): ReLU()\n","              (offset_conv1): Conv2d(160, 216, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","              (deform1): DeformConv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8)\n","              (SA): SpatialAttention(\n","                (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n","                (sigmoid): Sigmoid()\n","              )\n","            )\n","          )\n","          (attn_drop): Dropout(p=0.0, inplace=False)\n","          (proj): Linear(in_features=160, out_features=160, bias=True)\n","          (se_layer): Identity()\n","          (proj_drop): Dropout(p=0.0, inplace=False)\n","          (softmax): Softmax(dim=-1)\n","        )\n","        (drop_path): DropPath(drop_prob=0.071)\n","        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n","        (mlp): Mlp(\n","          (fc1): Linear(in_features=160, out_features=640, bias=True)\n","          (act): GELU(approximate='none')\n","          (fc2): Linear(in_features=640, out_features=160, bias=True)\n","          (drop): Dropout(p=0.0, inplace=False)\n","        )\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":26}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3735,"status":"ok","timestamp":1696476899598,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"iYN12YH5OLhN","outputId":"beaca0fb-6d21-462a-c78c-ba20574bf3bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n"]}],"source":["!pip install tqdm"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1696476899598,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"IpYrHzzNOY81"},"outputs":[],"source":["from tqdm import tqdm\n","import math"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":639,"status":"ok","timestamp":1696476900228,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"rwre64I_Opur"},"outputs":[],"source":["def expand2square(timg,factor=32.0):\n","    timg = timg.unsqueeze(0)\n","    _, _, h, w = timg.size()\n","\n","    X = int(math.ceil(max(h,w)/float(factor))*factor)\n","\n","    img = torch.zeros(1,3,X,X).type_as(timg) # 3, h,w\n","    mask = torch.zeros(1,1,X,X).type_as(timg)\n","\n","\n","    img[:,:, ((X - h)//2):((X - h)//2 + h),((X - w)//2):((X - w)//2 + w)] = timg\n","    mask[:,:, ((X - h)//2):((X - h)//2 + h),((X - w)//2):((X - w)//2 + w)].fill_(1.0)\n","\n","    return img, mask"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476900228,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"1CX-VbP88A41"},"outputs":[],"source":["#print(data_loader[0])"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696476900228,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"uGnDZv4tg23G"},"outputs":[],"source":["# data_loader[0][0]"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1696476900229,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"QYwlzLzcjULw"},"outputs":[],"source":["from torch.optim.lr_scheduler import CosineAnnealingLR"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6800,"status":"ok","timestamp":1696476907024,"user":{"displayName":"Anh Phạm Thị Trâm","userId":"15461037140885299192"},"user_tz":-420},"id":"tAQ5n-y61HxS","outputId":"1fb81668-3c4c-4ea1-c611-554e53c855ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Adjusting learning rate of group 0 to 1.9706e-04.\n"]}],"source":["checkpoint = torch.load('/content/drive/MyDrive/AIDTransformer/checkpoint.pth')\n","model.load_state_dict(checkpoint['state_dict'])\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.0002)\n","optimizer.load_state_dict(checkpoint['optimizer'])\n","epoch = checkpoint['epoch']\n","scheduler = CosineAnnealingLR(optimizer, T_max = 100, verbose = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dTGgCvq_N5Se","outputId":"052873e4-68d9-4729-c295-45e23295bf28"},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/736 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 0\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 1/736 [00:13<2:46:46, 13.61s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 1\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 2/736 [00:17<1:38:39,  8.06s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 2\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 3/736 [00:22<1:19:33,  6.51s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 3\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 4/736 [00:28<1:16:23,  6.26s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 4\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 5/736 [00:33<1:10:20,  5.77s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 5\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 6/736 [00:38<1:06:08,  5.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 6\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 7/736 [00:47<1:23:48,  6.90s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 7\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 8/736 [00:51<1:12:42,  5.99s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 8\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|          | 9/736 [00:56<1:05:08,  5.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 9\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 10/736 [01:00<59:53,  4.95s/it] "]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 10\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  1%|▏         | 11/736 [01:03<56:16,  4.66s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 11\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 12/736 [01:07<53:44,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 12\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 13/736 [01:12<53:05,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 13\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 14/736 [01:16<51:30,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 14\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 15/736 [01:20<50:37,  4.21s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 15\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 16/736 [01:24<50:11,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 16\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 17/736 [01:28<50:07,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 17\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 18/736 [01:32<49:46,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 18\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 19/736 [01:36<49:44,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 19\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 20/736 [01:41<49:39,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 20\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 21/736 [01:45<49:34,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 21\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 22/736 [01:49<49:37,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 22\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 23/736 [01:53<49:48,  4.19s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 23\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 24/736 [01:57<49:24,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 24\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  3%|▎         | 25/736 [02:01<49:28,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 25\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 26/736 [02:05<48:30,  4.10s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 26\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 27/736 [02:10<49:06,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 27\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 28/736 [02:14<49:33,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 28\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 29/736 [02:18<49:46,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 29\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 30/736 [02:22<49:47,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 30\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 31/736 [02:27<49:52,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 31\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 32/736 [02:31<49:50,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 32\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 33/736 [02:35<49:50,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 33\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 34/736 [02:39<49:22,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 34\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 35/736 [02:43<48:08,  4.12s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 35\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▍         | 36/736 [02:48<48:29,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 36\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 37/736 [02:52<48:28,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 37\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 38/736 [02:56<48:48,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 38\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 39/736 [03:00<48:37,  4.19s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 39\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 40/736 [03:04<48:20,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 40\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 41/736 [03:09<48:31,  4.19s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 41\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 42/736 [03:13<48:33,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 42\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 43/736 [03:17<49:39,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 43\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 44/736 [03:22<50:12,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 44\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▌         | 45/736 [03:26<51:05,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 45\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 46/736 [03:31<51:13,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 46\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  6%|▋         | 47/736 [03:35<49:14,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 47\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 48/736 [03:39<49:54,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 48\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 49/736 [03:44<50:05,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 49\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 50/736 [03:48<49:33,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 50\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 51/736 [03:52<50:03,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 51\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 52/736 [03:57<50:36,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 52\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 53/736 [04:02<50:43,  4.46s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 53\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 54/736 [04:06<51:07,  4.50s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 54\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 55/736 [04:10<50:10,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 55\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 56/736 [04:15<51:09,  4.51s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 56\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 57/736 [04:20<50:57,  4.50s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 57\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 58/736 [04:24<49:49,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 58\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 59/736 [04:28<49:04,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 59\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 60/736 [04:32<49:32,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 60\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 61/736 [04:37<51:05,  4.54s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 61\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 62/736 [04:42<50:10,  4.47s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 62\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 63/736 [04:46<50:17,  4.48s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 63\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▊         | 64/736 [04:51<50:10,  4.48s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 64\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 65/736 [04:55<50:14,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 65\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 66/736 [05:00<50:12,  4.50s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 66\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 67/736 [05:04<49:10,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 67\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 68/736 [05:08<48:32,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 68\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 69/736 [05:12<47:59,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 69\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 70/736 [05:17<48:51,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 70\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 71/736 [05:21<49:01,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 71\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 72/736 [05:26<49:14,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 72\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|▉         | 73/736 [05:30<49:23,  4.47s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 73\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 74/736 [05:35<49:34,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 74\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 75/736 [05:39<48:54,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 75\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 76/736 [05:44<48:55,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 76\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 10%|█         | 77/736 [05:48<48:41,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 77\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 78/736 [05:53<49:13,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 78\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 79/736 [05:57<49:12,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 79\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 80/736 [06:02<48:21,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 80\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 81/736 [06:06<47:37,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 81\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 82/736 [06:10<48:00,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 82\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█▏        | 83/736 [06:15<47:32,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 83\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█▏        | 84/736 [06:19<46:58,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 84\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 85/736 [06:23<47:23,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 85\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 86/736 [06:28<47:03,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 86\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 87/736 [06:32<46:58,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 87\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 88/736 [06:37<47:51,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 88\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 89/736 [06:41<47:19,  4.39s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 89\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 90/736 [06:45<47:52,  4.45s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 90\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 91/736 [06:50<46:43,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 91\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▎        | 92/736 [06:54<46:18,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 92\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 93/736 [06:58<46:45,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 93\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 94/736 [07:02<46:16,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 94\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 95/736 [07:07<46:05,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 95\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 96/736 [07:11<46:59,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 96\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 97/736 [07:16<46:30,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 97\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 98/736 [07:20<46:50,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 98\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 99/736 [07:24<46:38,  4.39s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 99\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▎        | 100/736 [07:29<46:50,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 100\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▎        | 101/736 [07:33<46:51,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 101\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 102/736 [07:38<46:08,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 102\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 103/736 [07:42<46:39,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 103\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 104/736 [07:46<46:09,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 104\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 105/736 [07:51<46:18,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 105\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 14%|█▍        | 106/736 [07:55<46:33,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 106\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 107/736 [08:00<45:53,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 107\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 108/736 [08:04<46:22,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 108\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 109/736 [08:08<45:40,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 109\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 110/736 [08:13<46:10,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 110\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 111/736 [08:17<45:34,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 111\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 112/736 [08:22<46:05,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 112\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 113/736 [08:26<45:33,  4.39s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 113\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▌        | 114/736 [08:30<44:55,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 114\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 115/736 [08:35<44:25,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 115\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 116/736 [08:39<45:51,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 116\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 117/736 [08:44<45:08,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 117\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 118/736 [08:48<44:45,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 118\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 119/736 [08:52<44:35,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 119\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 120/736 [08:56<44:12,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 120\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▋        | 121/736 [09:01<44:25,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 121\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 122/736 [09:05<44:59,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 122\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 123/736 [09:10<45:09,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 123\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 124/736 [09:14<45:31,  4.46s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 124\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 125/736 [09:19<44:27,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 125\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 126/736 [09:23<44:05,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 126\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 127/736 [09:27<43:43,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 127\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 17%|█▋        | 128/736 [09:31<43:27,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 128\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 129/736 [09:35<43:09,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 129\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 130/736 [09:40<42:58,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 130\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 131/736 [09:44<43:05,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 131\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 132/736 [09:49<43:41,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 132\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 133/736 [09:53<43:16,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 133\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 134/736 [09:57<43:19,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 134\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 135/736 [10:02<44:13,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 135\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 18%|█▊        | 136/736 [10:06<43:32,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 136\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▊        | 137/736 [10:11<44:08,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 137\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 138/736 [10:15<43:28,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 138\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 139/736 [10:19<43:12,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 139\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 140/736 [10:23<42:59,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 140\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 141/736 [10:28<43:40,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 141\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 142/736 [10:32<43:54,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 142\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 19%|█▉        | 143/736 [10:37<43:26,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 143\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 144/736 [10:41<42:38,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 144\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 145/736 [10:45<42:23,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 145\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 146/736 [10:49<42:06,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 146\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|█▉        | 147/736 [10:54<42:45,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 147\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 148/736 [10:59<43:48,  4.47s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 148\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 149/736 [11:03<43:00,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 149\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 150/736 [11:07<43:09,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 150\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 151/736 [11:12<42:37,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 151\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 152/736 [11:16<42:40,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 152\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 153/736 [11:20<42:18,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 153\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 154/736 [11:25<42:02,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 154\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 155/736 [11:29<42:02,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 155\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██        | 156/736 [11:33<41:26,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 156\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██▏       | 157/736 [11:38<42:11,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 157\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 21%|██▏       | 158/736 [11:42<42:34,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 158\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 159/736 [11:46<41:58,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 159\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 160/736 [11:51<41:25,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 160\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 161/736 [11:55<41:20,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 161\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 162/736 [11:59<40:53,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 162\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 163/736 [12:03<41:00,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 163\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 164/736 [12:08<40:36,  4.26s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 164\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 22%|██▏       | 165/736 [12:12<40:40,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 165\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 166/736 [12:16<40:04,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 166\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 167/736 [12:20<39:53,  4.21s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 167\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 168/736 [12:25<40:08,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 168\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 169/736 [12:29<40:11,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 169\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 170/736 [12:33<40:21,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 170\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 171/736 [12:38<41:29,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 171\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 23%|██▎       | 172/736 [12:42<40:52,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 172\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▎       | 173/736 [12:46<40:32,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 173\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▎       | 174/736 [12:51<40:10,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 174\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 175/736 [12:54<39:00,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 175\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 176/736 [12:59<39:12,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 176\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 177/736 [13:03<39:21,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 177\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 178/736 [13:07<39:34,  4.26s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 178\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 179/736 [13:12<40:14,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 179\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 180/736 [13:16<39:42,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 180\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 181/736 [13:21<40:17,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 181\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 182/736 [13:25<40:20,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 182\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▍       | 183/736 [13:29<40:17,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 183\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 184/736 [13:34<40:26,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 184\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 185/736 [13:38<39:43,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 185\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 186/736 [13:42<39:10,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 186\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 25%|██▌       | 187/736 [13:46<38:42,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 187\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 188/736 [13:50<38:46,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 188\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 189/736 [13:55<39:03,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 189\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 190/736 [13:59<39:14,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 190\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 191/736 [14:04<39:20,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 191\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 192/736 [14:08<39:00,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 192\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▌       | 193/736 [14:12<38:43,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 193\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▋       | 194/736 [14:17<39:24,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 194\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 26%|██▋       | 195/736 [14:21<38:55,  4.32s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 195\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 196/736 [14:25<38:32,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 196\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 197/736 [14:29<38:14,  4.26s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 197\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 198/736 [14:34<38:18,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 198\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 199/736 [14:38<38:13,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 199\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 200/736 [14:42<37:13,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 200\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 201/736 [14:46<38:02,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 201\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 27%|██▋       | 202/736 [14:51<38:42,  4.35s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 202\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 203/736 [14:55<38:35,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 203\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 204/736 [14:59<38:02,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 204\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 205/736 [15:04<37:51,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 205\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 206/736 [15:08<38:18,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 206\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 207/736 [15:12<38:09,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 207\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 208/736 [15:17<38:14,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 208\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 209/736 [15:21<37:38,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 209\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 210/736 [15:25<37:59,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 210\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▊       | 211/736 [15:29<37:23,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 211\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 212/736 [15:34<37:28,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 212\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 213/736 [15:38<37:22,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 213\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 214/736 [15:42<37:09,  4.27s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 214\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 215/736 [15:47<37:14,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 215\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 216/736 [15:51<36:52,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 216\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 29%|██▉       | 217/736 [15:55<36:37,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 217\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 218/736 [15:59<36:32,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 218\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 219/736 [16:03<36:20,  4.22s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 219\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|██▉       | 220/736 [16:08<36:28,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 220\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 221/736 [16:12<36:29,  4.25s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 221\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 222/736 [16:16<36:19,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 222\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 223/736 [16:20<35:25,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 223\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 30%|███       | 224/736 [16:24<35:27,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 224\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 225/736 [16:29<35:40,  4.19s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 225\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 226/736 [16:33<35:46,  4.21s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 226\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 227/736 [16:37<34:45,  4.10s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 227\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 228/736 [16:40<34:01,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 228\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███       | 229/736 [16:44<33:25,  3.96s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 229\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███▏      | 230/736 [16:48<33:06,  3.93s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 230\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 31%|███▏      | 231/736 [16:52<33:37,  3.99s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 231\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 232/736 [16:57<34:45,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 232\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 233/736 [17:01<34:43,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 233\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 234/736 [17:05<34:03,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 234\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 235/736 [17:09<33:31,  4.01s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 235\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 236/736 [17:13<33:58,  4.08s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 236\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 237/736 [17:17<34:41,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 237\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 238/736 [17:21<33:48,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 238\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 239/736 [17:25<33:16,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 239\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 240/736 [17:29<32:48,  3.97s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 240\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 241/736 [17:33<32:25,  3.93s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 241\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 242/736 [17:37<32:09,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 242\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 243/736 [17:41<32:50,  4.00s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 243\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 244/736 [17:45<33:33,  4.09s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 244\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 245/736 [17:49<34:11,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 245\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 246/736 [17:54<34:39,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 246\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 247/736 [17:58<35:06,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 247\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▎      | 248/736 [18:02<33:56,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 248\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 249/736 [18:06<33:13,  4.09s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 249\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 250/736 [18:10<32:31,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 250\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 251/736 [18:14<32:06,  3.97s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 251\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 252/736 [18:18<31:47,  3.94s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 252\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 34%|███▍      | 253/736 [18:22<31:56,  3.97s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 253\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 254/736 [18:26<31:41,  3.94s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 254\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 255/736 [18:30<32:16,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 255\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 256/736 [18:34<32:35,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 256\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▍      | 257/736 [18:38<32:43,  4.10s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 257\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 258/736 [18:42<32:58,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 258\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 259/736 [18:46<32:13,  4.05s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 259\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 260/736 [18:50<31:37,  3.99s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 260\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 35%|███▌      | 261/736 [18:54<31:16,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 261\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 262/736 [18:58<31:04,  3.93s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 262\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 263/736 [19:02<30:55,  3.92s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 263\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 264/736 [19:06<30:42,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 264\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 265/736 [19:09<30:39,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 265\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 266/736 [19:14<31:13,  3.99s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 266\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 267/736 [19:18<31:31,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 267\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▋      | 268/736 [19:22<31:53,  4.09s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 268\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 269/736 [19:26<32:13,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 269\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 270/736 [19:30<31:24,  4.04s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 270\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 271/736 [19:34<30:50,  3.98s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 271\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 272/736 [19:38<30:39,  3.96s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 272\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 273/736 [19:42<30:30,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 273\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 274/736 [19:46<30:14,  3.93s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 274\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 37%|███▋      | 275/736 [19:49<29:59,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 275\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 276/736 [19:53<29:53,  3.90s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 276\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 277/736 [19:58<31:28,  4.11s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 277\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 278/736 [20:02<31:44,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 278\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 279/736 [20:06<31:47,  4.17s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 279\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 280/736 [20:11<32:21,  4.26s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 280\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 281/736 [20:15<31:22,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 281\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 282/736 [20:19<30:45,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 282\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 38%|███▊      | 283/736 [20:23<30:15,  4.01s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 283\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▊      | 284/736 [20:27<30:40,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 284\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▊      | 285/736 [20:31<30:45,  4.09s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 285\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 286/736 [20:35<30:08,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 286\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 287/736 [20:39<29:48,  3.98s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 287\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 288/736 [20:43<30:23,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 288\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 289/736 [20:47<30:40,  4.12s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 289\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 39%|███▉      | 290/736 [20:51<30:09,  4.06s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 290\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 291/736 [20:55<30:33,  4.12s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 291\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 292/736 [20:59<29:55,  4.04s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 292\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 293/736 [21:03<30:10,  4.09s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 293\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|███▉      | 294/736 [21:07<29:38,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 294\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 295/736 [21:11<29:23,  4.00s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 295\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 296/736 [21:15<29:49,  4.07s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 296\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 297/736 [21:19<29:27,  4.03s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 297\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 298/736 [21:23<29:01,  3.98s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 298\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 299/736 [21:28<30:03,  4.13s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 299\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 300/736 [21:32<30:22,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 300\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 301/736 [21:36<30:17,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 301\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 302/736 [21:41<30:56,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 302\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████      | 303/736 [21:45<30:00,  4.16s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 303\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 304/736 [21:49<29:31,  4.10s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 304\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 41%|████▏     | 305/736 [21:52<28:50,  4.02s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 305\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 306/736 [21:57<29:40,  4.14s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 306\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 307/736 [22:01<30:02,  4.20s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 307\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 308/736 [22:05<29:50,  4.18s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 308\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 309/736 [22:10<30:06,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 309\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 310/736 [22:14<30:02,  4.23s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 310\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 311/736 [22:18<30:03,  4.24s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 311\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 42%|████▏     | 312/736 [22:22<29:45,  4.21s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 312\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 313/736 [22:27<30:17,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 313\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 314/736 [22:31<30:13,  4.30s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 314\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 315/736 [22:35<30:00,  4.28s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 315\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 316/736 [22:40<30:03,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 316\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 317/736 [22:44<30:16,  4.33s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 317\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 318/736 [22:49<30:55,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 318\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 319/736 [22:53<30:22,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 319\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 43%|████▎     | 320/736 [22:57<30:06,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 320\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▎     | 321/736 [23:02<31:02,  4.49s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 321\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 322/736 [23:06<30:38,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 322\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 323/736 [23:11<30:19,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 323\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 324/736 [23:15<30:02,  4.38s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 324\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 325/736 [23:20<30:13,  4.41s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 325\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 326/736 [23:24<29:49,  4.36s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 326\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 327/736 [23:28<30:06,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 327\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 328/736 [23:33<30:34,  4.50s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 328\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 329/736 [23:38<30:33,  4.51s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 329\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 330/736 [23:42<30:36,  4.52s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 330\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▍     | 331/736 [23:46<29:57,  4.44s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 331\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 332/736 [23:51<29:44,  4.42s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 332\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 333/736 [23:55<29:29,  4.39s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 333\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 45%|████▌     | 334/736 [23:59<29:18,  4.37s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 334\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 335/736 [24:04<29:01,  4.34s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 335\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 336/736 [24:08<28:43,  4.31s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 336\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 337/736 [24:12<28:32,  4.29s/it]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([3, 256, 256])\n","14 337\n","1\n","torch.Size([1, 3, 256, 256])\n","torch.Size([1, 3, 256, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\r 46%|████▌     | 338/736 [24:17<28:55,  4.36s/it]"]}],"source":["# optimizer = torch.optim.Adam(model.parameters(), lr = 0.0002)\n","# scheduler = CosineAnnealingLR(optimizer, T_max = 100, verbose = True)\n","\n","epochs = 120\n","#final_loss = []\n","for i in range(epoch+1, epochs):\n","  #for ii in range(len(data_loader)):\n","  for ii, data_train in enumerate(tqdm(data_loader), 0):\n","    print(i, ii)\n","    rgb_gt = data_train[0].unsqueeze(0).to('cuda')\n","\n","    rgb_noisy, mask = expand2square(data_train[1].cuda(), factor=128)\n","    rgb_noisy, mask = rgb_noisy.to('cuda'), mask.to('cuda')\n","\n","    optimizer.zero_grad()\n","\n","    # print(rgb_gt)\n","    # Forward pass\n","    outputs = model(rgb_noisy)\n","    #outputs = model(rgb_noisy, 1 - mask)\n","\n","    # Compute loss\n","    loss = loss_function(outputs, rgb_gt)\n","    # final_loss.append(loss)\n","\n","    # Backward pass and optimize\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","    if ii == 735:\n","      break\n","\n","  scheduler.step()\n","  print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n","\n","  torch.save({\n","            'epoch': i,\n","            'state_dict': model.state_dict(),\n","            'optimizer': optimizer.state_dict(),\n","            #'loss': loss,\n","            }, '/content/drive/MyDrive/AIDTransformer/checkpoint.pth')\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_EbTgvo7Qr0"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}